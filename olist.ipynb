{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = './data'\n",
    "CLOSED_DATA_FILENAME = 'olist_closed_deals_dataset.csv'\n",
    "QUALIFIED_DATA_FILENAME = 'olist_marketing_qualified_leads_dataset.csv'\n",
    "\n",
    "RESOURCES_DIR_PATH = './resources'\n",
    "LANDING_PAGE_ID_VOCABULARY_FILENAME = 'landing_page_id_vocabulary.txt'\n",
    "ORIGIN_VOCABULARY_FILENAME = 'origin.txt'\n",
    "PROCESSED_TRAIN_DATA_FILENAME = 'processed_train_data.csv'\n",
    "PROCESSED_TEST_DATA_FILENAME = 'processed_test_data.csv'\n",
    "\n",
    "SPLIT_RATIO = 0.8\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "EPOCHS = 100\n",
    "STEPS_PER_EPOCH = 20\n",
    "\n",
    "DEFAULT_NAN_STRING = 'missing'\n",
    "DEFAULT_NAN_INT = -1\n",
    "\n",
    "MODEL_DIR = './models'\n",
    "MODEL_NAME = 'olist_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_paths():\n",
    "    data_dir = pathlib.Path(DATA_DIR_PATH)\n",
    "    assert data_dir.exists() and data_dir.is_dir()\n",
    "\n",
    "    closed_deals_path = data_dir / CLOSED_DATA_FILENAME\n",
    "    assert closed_deals_path.exists() and closed_deals_path.is_file()\n",
    "\n",
    "    qualified_deals_path = data_dir / QUALIFIED_DATA_FILENAME\n",
    "    assert qualified_deals_path.exists() and qualified_deals_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    data_dir = pathlib.Path(DATA_DIR_PATH)\n",
    "\n",
    "    closed_deals_path = data_dir / CLOSED_DATA_FILENAME\n",
    "    closed_df = pd.read_csv(closed_deals_path)\n",
    "\n",
    "    qualified_deals_path = data_dir / QUALIFIED_DATA_FILENAME\n",
    "    qualified_df = pd.read_csv(qualified_deals_path)\n",
    "\n",
    "    raw_df = qualified_df.merge(closed_df, on='mql_id', how='left')\n",
    "\n",
    "    filtered_df = raw_df[['landing_page_id', 'origin']]\n",
    "    filtered_df['origin'] = filtered_df['origin'].fillna(DEFAULT_NAN_STRING)\n",
    "    filtered_df['label'] = raw_df['seller_id'].notna()\n",
    "\n",
    "    resources_dir = pathlib.Path(RESOURCES_DIR_PATH)\n",
    "    if not resources_dir.exists():\n",
    "        resources_dir.mkdir()\n",
    "\n",
    "    landing_page_id_vocabulary_file = resources_dir / \\\n",
    "        LANDING_PAGE_ID_VOCABULARY_FILENAME\n",
    "    if landing_page_id_vocabulary_file.exists():\n",
    "        landing_page_id_vocabulary_file.unlink()\n",
    "\n",
    "    origin_vocabulary_file = resources_dir / ORIGIN_VOCABULARY_FILENAME\n",
    "    if origin_vocabulary_file.exists():\n",
    "        origin_vocabulary_file.unlink()\n",
    "\n",
    "    np.savetxt(landing_page_id_vocabulary_file,\n",
    "               pd.unique(filtered_df['landing_page_id']), fmt='%s')\n",
    "    np.savetxt(origin_vocabulary_file,\n",
    "               pd.unique(filtered_df['origin']), fmt='%s')\n",
    "\n",
    "    filtered_df = filtered_df.sample(frac=1).reset_index(drop=True)\n",
    "    split = int(len(filtered_df) * SPLIT_RATIO)\n",
    "\n",
    "    train_df = filtered_df[:split]\n",
    "    test_df = filtered_df[split:]\n",
    "\n",
    "    processed_train_data_file = resources_dir / PROCESSED_TRAIN_DATA_FILENAME\n",
    "    if processed_train_data_file.exists():\n",
    "        processed_train_data_file.unlink()\n",
    "\n",
    "    processed_test_data_file = resources_dir / PROCESSED_TEST_DATA_FILENAME\n",
    "    if processed_test_data_file.exists():\n",
    "        processed_test_data_file.unlink()\n",
    "\n",
    "    train_df.to_csv(processed_train_data_file, index=False, header=True)\n",
    "    test_df.to_csv(processed_test_data_file, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_preprocess_step():\n",
    "    resources_dir = pathlib.Path(RESOURCES_DIR_PATH)\n",
    "    assert resources_dir.exists()\n",
    "\n",
    "    landing_page_id_vocabulary_file = resources_dir / \\\n",
    "        LANDING_PAGE_ID_VOCABULARY_FILENAME\n",
    "    assert landing_page_id_vocabulary_file.exists()\n",
    "\n",
    "    origin_vocabulary_file = resources_dir / ORIGIN_VOCABULARY_FILENAME\n",
    "    assert origin_vocabulary_file.exists()\n",
    "\n",
    "    processed_train_data_file = resources_dir / PROCESSED_TRAIN_DATA_FILENAME\n",
    "    assert processed_train_data_file.exists()\n",
    "\n",
    "    processed_test_data_file = resources_dir / PROCESSED_TEST_DATA_FILENAME\n",
    "    assert processed_test_data_file.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    resources_dir = pathlib.Path(RESOURCES_DIR_PATH)\n",
    "    processed_train_data_file = resources_dir / PROCESSED_TRAIN_DATA_FILENAME\n",
    "    processed_test_data_file = resources_dir / PROCESSED_TEST_DATA_FILENAME\n",
    "\n",
    "    train_df = pd.read_csv(processed_train_data_file)\n",
    "    test_df = pd.read_csv(processed_test_data_file)\n",
    "\n",
    "    train_x, train_y = train_df, train_df.pop('label')\n",
    "    test_x, test_y = test_df, test_df.pop('label')\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    dataset = dataset.shuffle(len(features)).repeat().batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_from_data(labels):\n",
    "    total = len(labels)\n",
    "    negative, positive = np.bincount(labels)\n",
    "\n",
    "    weight_for_0 = (1 / negative) * (total) / 2.0\n",
    "    weight_for_1 = (1 / positive) * (total) / 2.0\n",
    "\n",
    "    class_weight = {False: weight_for_0, True: weight_for_1}\n",
    "\n",
    "    output_bias = np.log([positive / negative])\n",
    "\n",
    "    return class_weight, output_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(output_bias=None):\n",
    "    resources_dir = pathlib.Path(RESOURCES_DIR_PATH)\n",
    "    landing_page_id_vocabulary_file = resources_dir / \\\n",
    "        LANDING_PAGE_ID_VOCABULARY_FILENAME\n",
    "    origin_vocabulary_file = resources_dir / ORIGIN_VOCABULARY_FILENAME\n",
    "\n",
    "    landing_page_id_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        key='landing_page_id',\n",
    "        vocabulary_file=str(landing_page_id_vocabulary_file),\n",
    "        dtype=tf.dtypes.string,\n",
    "        num_oov_buckets=1\n",
    "    )\n",
    "\n",
    "    origin_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        key='origin',\n",
    "        vocabulary_file=str(origin_vocabulary_file),\n",
    "        dtype=tf.dtypes.string,\n",
    "        num_oov_buckets=1\n",
    "    )\n",
    "\n",
    "    feature_columns = [tf.feature_column.indicator_column(landing_page_id_column),\n",
    "                       tf.feature_column.indicator_column(origin_column)]\n",
    "    \n",
    "    feature_layer_inputs = {}\n",
    "    feature_layer_inputs['landing_page_id'] = tf.keras.Input(shape=(1,), name='landing_page_id', dtype=tf.string)\n",
    "    feature_layer_inputs['origin'] = tf.keras.Input(shape=(1,), name='origin', dtype=tf.string)\n",
    "\n",
    "    if output_bias:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    \n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    dense = feature_layer(feature_layer_inputs)\n",
    "    \n",
    "    dense = tf.keras.layers.BatchNormalization()(dense)\n",
    "    dense = tf.keras.layers.Dense(10, activation='relu')(dense)\n",
    "    dense = tf.keras.layers.BatchNormalization()(dense)\n",
    "    dense = tf.keras.layers.Dense(10, activation='relu')(dense)\n",
    "    dense = tf.keras.layers.BatchNormalization()(dense)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)(dense)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=output)\n",
    "\n",
    "    metrics = [\n",
    "        tf.keras.metrics.TruePositives(name='tp'),\n",
    "        tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    (train_x, train_y), (test_x, test_y) = load_data()\n",
    "    train_dataset = get_dataset(train_x, train_y, BATCH_SIZE)\n",
    "    test_dataset = get_dataset(test_x, test_y, BATCH_SIZE)\n",
    "\n",
    "    class_weight, output_bias = get_stats_from_data(train_y)\n",
    "\n",
    "    model = get_model(output_bias)\n",
    "\n",
    "    history = model.fit(train_dataset, epochs=EPOCHS,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH, class_weight=class_weight, verbose=0)\n",
    "\n",
    "    model_dir = pathlib.Path(MODEL_DIR)\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir()\n",
    "\n",
    "    model.save(model_dir / MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_train():\n",
    "    model_dir = pathlib.Path(MODEL_DIR)\n",
    "    assert model_dir.exists()\n",
    "\n",
    "    saved_model_dir = model_dir / MODEL_NAME\n",
    "    assert saved_model_dir.exists()\n",
    "\n",
    "    assert (saved_model_dir / 'assets').exists()\n",
    "    assert (saved_model_dir / 'variables').exists()\n",
    "    assert (saved_model_dir / 'saved_model.pb').exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    validate_data_paths()\n",
    "    preprocess()\n",
    "    validate_preprocess_step()\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.set_tag(\"version.mlflow\", mlflow.__version__)\n",
    "        mlflow.set_tag(\"version.keras\", tf.keras.__version__)\n",
    "        mlflow.set_tag(\"version.tensorflow\", tf.__version__)\n",
    "\n",
    "        train()\n",
    "        validate_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}